{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2065f8c7-5c35-4a35-8d3c-40a3fe4bb79c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-924525323806180>, line 19\u001B[0m\n",
       "\u001B[1;32m     16\u001B[0m mount_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m mount_name\n",
       "\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Montar o bucket S3 como ponto de montagem\u001B[39;00m\n",
       "\u001B[0;32m---> 19\u001B[0m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mmount(source \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ms3a://\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m bucket_name, \n",
       "\u001B[1;32m     20\u001B[0m                  mount_point \u001B[38;5;241m=\u001B[39m mount_path)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:362\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    360\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    361\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m--> 362\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
       "\n",
       "\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o4594.mount.\n",
       ": java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/s3-ailos-teste; nested exception is: \n",
       "\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/s3-ailos-teste\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1053)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1079)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:667)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:685)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:71)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:71)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:662)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:71)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:71)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:135)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1073)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/s3-ailos-teste\n",
       "\tat scala.Predef$.require(Predef.scala:281)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:704)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:1080)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:853)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:1069)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:712)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:128)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$10(DbfsServerBackend.scala:436)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:436)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:335)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:512)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:615)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:633)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:610)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:521)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:513)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:481)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:545)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:59)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:545)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:72)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:521)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:180)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$4(InstrumentedQueuedThreadPool.scala:104)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:47)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:104)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:67)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:64)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:47)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:86)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:840)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-924525323806180>, line 19\u001B[0m\n\u001B[1;32m     16\u001B[0m mount_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m mount_name\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Montar o bucket S3 como ponto de montagem\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mmount(source \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ms3a://\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m bucket_name, \n\u001B[1;32m     20\u001B[0m                  mount_point \u001B[38;5;241m=\u001B[39m mount_path)\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:362\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    360\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    361\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 362\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n\n\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o4594.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/s3-ailos-teste; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/s3-ailos-teste\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1053)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1079)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:667)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:685)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:662)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:71)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:135)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1073)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/s3-ailos-teste\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:704)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:1080)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:853)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:1069)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:712)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:128)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$10(DbfsServerBackend.scala:436)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:436)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:335)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:512)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:615)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:633)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:610)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:521)\n\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:513)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:481)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:545)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:59)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:545)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:72)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:521)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:180)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$4(InstrumentedQueuedThreadPool.scala:104)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:47)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:104)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:67)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:64)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:47)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:86)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n\tat java.lang.Thread.run(Thread.java:840)\n",
       "errorSummary": "java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/s3-ailos-teste; nested exception is: ",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configurar as credenciais do AWS S3\n",
    "aws_access_key_id = \"AKIAZI2LIFQPPP5COS4M\"\n",
    "aws_secret_access_key = \"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\"\n",
    "\n",
    "# Configurar as credenciais no ambiente\n",
    "spark.conf.set(\"fs.s3a.access.key\", aws_access_key_id)\n",
    "spark.conf.set(\"fs.s3a.secret.key\", aws_secret_access_key)\n",
    "\n",
    "# Especificar o bucket S3 a ser montado\n",
    "bucket_name = \"ailos-teste\"\n",
    "\n",
    "# Especificar o ponto de montagem no Databricks\n",
    "mount_name = \"s3-ailos-teste\"\n",
    "\n",
    "# Caminho no bucket S3 que deseja montar\n",
    "mount_path = \"/mnt/\" + mount_name\n",
    "\n",
    "# Montar o bucket S3 como ponto de montagem\n",
    "dbutils.fs.mount(source = \"s3a://\" + bucket_name, \n",
    "                 mount_point = mount_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e63f410-ebc4-4ff0-8c76-b22bd4078856",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/mnt/s3-ailos-teste/CEXT_7562011_20240125_0002504.CCB', name='CEXT_7562011_20240125_0002504.CCB', size=3067, modificationTime=1715047033000),\n",
       " FileInfo(path='dbfs:/mnt/s3-ailos-teste/DDLs.sql', name='DDLs.sql', size=14916, modificationTime=1715047032000),\n",
       " FileInfo(path='dbfs:/mnt/s3-ailos-teste/Estoria de Usuario.docx', name='Estoria de Usuario.docx', size=190307, modificationTime=1715047031000),\n",
       " FileInfo(path='dbfs:/mnt/s3-ailos-teste/TB_ARQUIVO/', name='TB_ARQUIVO/', size=0, modificationTime=1715125661248),\n",
       " FileInfo(path='dbfs:/mnt/s3-ailos-teste/TB_ARQUIVO_CONTROLE/', name='TB_ARQUIVO_CONTROLE/', size=0, modificationTime=1715125661248),\n",
       " FileInfo(path='dbfs:/mnt/s3-ailos-teste/TB_ARQUIVO_LINHA/', name='TB_ARQUIVO_LINHA/', size=0, modificationTime=1715125661248),\n",
       " FileInfo(path='dbfs:/mnt/s3-ailos-teste/TB_CARTAO/', name='TB_CARTAO/', size=0, modificationTime=1715125661248),\n",
       " FileInfo(path='dbfs:/mnt/s3-ailos-teste/bronze/', name='bronze/', size=0, modificationTime=1715125661248),\n",
       " FileInfo(path='dbfs:/mnt/s3-ailos-teste/silver/', name='silver/', size=0, modificationTime=1715125661248)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listar arquivos no ponto de montagem\n",
    "dbutils.fs.ls(\"/mnt/s3-ailos-teste\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a4aab15-657a-414b-9c43-59bb8026dfff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNoCredentialsError\u001B[0m                        Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-924525323806191>, line 7\u001B[0m\n",
       "\u001B[1;32m      4\u001B[0m s3_client \u001B[38;5;241m=\u001B[39m boto3\u001B[38;5;241m.\u001B[39mclient(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms3\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Obtenha a localização do bucket\u001B[39;00m\n",
       "\u001B[0;32m----> 7\u001B[0m bucket_location \u001B[38;5;241m=\u001B[39m s3_client\u001B[38;5;241m.\u001B[39mget_bucket_location(Bucket\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mailos-teste\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Extraia a região do bucket\u001B[39;00m\n",
       "\u001B[1;32m     10\u001B[0m region \u001B[38;5;241m=\u001B[39m bucket_location[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLocationConstraint\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/botocore/client.py:508\u001B[0m, in \u001B[0;36mClientCreator._create_api_method.<locals>._api_call\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    504\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n",
       "\u001B[1;32m    505\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpy_operation_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m() only accepts keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    506\u001B[0m     )\n",
       "\u001B[1;32m    507\u001B[0m \u001B[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001B[39;00m\n",
       "\u001B[0;32m--> 508\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_api_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43moperation_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/botocore/client.py:898\u001B[0m, in \u001B[0;36mBaseClient._make_api_call\u001B[0;34m(self, operation_name, api_params)\u001B[0m\n",
       "\u001B[1;32m    896\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    897\u001B[0m     apply_request_checksum(request_dict)\n",
       "\u001B[0;32m--> 898\u001B[0m     http, parsed_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m    899\u001B[0m \u001B[43m        \u001B[49m\u001B[43moperation_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest_context\u001B[49m\n",
       "\u001B[1;32m    900\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    902\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmeta\u001B[38;5;241m.\u001B[39mevents\u001B[38;5;241m.\u001B[39memit(\n",
       "\u001B[1;32m    903\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mafter-call.\u001B[39m\u001B[38;5;132;01m{service_id}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{operation_name}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n",
       "\u001B[1;32m    904\u001B[0m         service_id\u001B[38;5;241m=\u001B[39mservice_id, operation_name\u001B[38;5;241m=\u001B[39moperation_name\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    909\u001B[0m     context\u001B[38;5;241m=\u001B[39mrequest_context,\n",
       "\u001B[1;32m    910\u001B[0m )\n",
       "\u001B[1;32m    912\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m300\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/botocore/client.py:921\u001B[0m, in \u001B[0;36mBaseClient._make_request\u001B[0;34m(self, operation_model, request_dict, request_context)\u001B[0m\n",
       "\u001B[1;32m    919\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_make_request\u001B[39m(\u001B[38;5;28mself\u001B[39m, operation_model, request_dict, request_context):\n",
       "\u001B[1;32m    920\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 921\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_endpoint\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43moperation_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest_dict\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    922\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    923\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmeta\u001B[38;5;241m.\u001B[39mevents\u001B[38;5;241m.\u001B[39memit(\n",
       "\u001B[1;32m    924\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mafter-call-error.\u001B[39m\u001B[38;5;132;01m{service_id}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{operation_name}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n",
       "\u001B[1;32m    925\u001B[0m                 service_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_service_model\u001B[38;5;241m.\u001B[39mservice_id\u001B[38;5;241m.\u001B[39mhyphenize(),\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    929\u001B[0m             context\u001B[38;5;241m=\u001B[39mrequest_context,\n",
       "\u001B[1;32m    930\u001B[0m         )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/botocore/endpoint.py:119\u001B[0m, in \u001B[0;36mEndpoint.make_request\u001B[0;34m(self, operation_model, request_dict)\u001B[0m\n",
       "\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmake_request\u001B[39m(\u001B[38;5;28mself\u001B[39m, operation_model, request_dict):\n",
       "\u001B[1;32m    114\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\n",
       "\u001B[1;32m    115\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMaking request for \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m with params: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    116\u001B[0m         operation_model,\n",
       "\u001B[1;32m    117\u001B[0m         request_dict,\n",
       "\u001B[1;32m    118\u001B[0m     )\n",
       "\u001B[0;32m--> 119\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moperation_model\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/botocore/endpoint.py:198\u001B[0m, in \u001B[0;36mEndpoint._send_request\u001B[0;34m(self, request_dict, operation_model)\u001B[0m\n",
       "\u001B[1;32m    196\u001B[0m context \u001B[38;5;241m=\u001B[39m request_dict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontext\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
       "\u001B[1;32m    197\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_retries_context(context, attempts)\n",
       "\u001B[0;32m--> 198\u001B[0m request \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moperation_model\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    199\u001B[0m success_response, exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_response(\n",
       "\u001B[1;32m    200\u001B[0m     request, operation_model, context\n",
       "\u001B[1;32m    201\u001B[0m )\n",
       "\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_needs_retry(\n",
       "\u001B[1;32m    203\u001B[0m     attempts,\n",
       "\u001B[1;32m    204\u001B[0m     operation_model,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    207\u001B[0m     exception,\n",
       "\u001B[1;32m    208\u001B[0m ):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/botocore/endpoint.py:134\u001B[0m, in \u001B[0;36mEndpoint.create_request\u001B[0;34m(self, params, operation_model)\u001B[0m\n",
       "\u001B[1;32m    130\u001B[0m     service_id \u001B[38;5;241m=\u001B[39m operation_model\u001B[38;5;241m.\u001B[39mservice_model\u001B[38;5;241m.\u001B[39mservice_id\u001B[38;5;241m.\u001B[39mhyphenize()\n",
       "\u001B[1;32m    131\u001B[0m     event_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrequest-created.\u001B[39m\u001B[38;5;132;01m{service_id}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{op_name}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n",
       "\u001B[1;32m    132\u001B[0m         service_id\u001B[38;5;241m=\u001B[39mservice_id, op_name\u001B[38;5;241m=\u001B[39moperation_model\u001B[38;5;241m.\u001B[39mname\n",
       "\u001B[1;32m    133\u001B[0m     )\n",
       "\u001B[0;32m--> 134\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_event_emitter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43memit\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m    135\u001B[0m \u001B[43m        \u001B[49m\u001B[43mevent_name\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    136\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    137\u001B[0m \u001B[43m        \u001B[49m\u001B[43moperation_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moperation_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    138\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    139\u001B[0m prepared_request \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_request(request)\n",
       "\u001B[1;32m    140\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m prepared_request\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/botocore/hooks.py:412\u001B[0m, in \u001B[0;36mEventAliaser.emit\u001B[0;34m(self, event_name, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    410\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21memit\u001B[39m(\u001B[38;5;28mself\u001B[39m, event_name, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n",
       "\u001B[1;32m    411\u001B[0m     aliased_event_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_alias_event_name(event_name)\n",
       "\u001B[0;32m--> 412\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_emitter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43memit\u001B[49m\u001B[43m(\u001B[49m\u001B[43maliased_event_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/botocore/hooks.py:256\u001B[0m, in \u001B[0;36mHierarchicalEmitter.emit\u001B[0;34m(self, event_name, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    245\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21memit\u001B[39m(\u001B[38;5;28mself\u001B[39m, event_name, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n",
       "\u001B[1;32m    246\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    247\u001B[0m \u001B[38;5;124;03m    Emit an event by name with arguments passed as keyword args.\u001B[39;00m\n",
       "\u001B[1;32m    248\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    254\u001B[0m \u001B[38;5;124;03m             handlers.\u001B[39;00m\n",
       "\u001B[1;32m    255\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 256\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_emit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevent_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/botocore/hooks.py:239\u001B[0m, in \u001B[0;36mHierarchicalEmitter._emit\u001B[0;34m(self, event_name, kwargs, stop_on_response)\u001B[0m\n",
       "\u001B[1;32m    237\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m handler \u001B[38;5;129;01min\u001B[39;00m handlers_to_call:\n",
       "\u001B[1;32m    238\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEvent \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m: calling handler \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m, event_name, handler)\n",
       "\u001B[0;32m--> 239\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mhandler\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    240\u001B[0m     responses\u001B[38;5;241m.\u001B[39mappend((handler, response))\n",
       "\u001B[1;32m    241\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stop_on_response \u001B[38;5;129;01mand\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/botocore/signers.py:103\u001B[0m, in \u001B[0;36mRequestSigner.handler\u001B[0;34m(self, operation_name, request, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhandler\u001B[39m(\u001B[38;5;28mself\u001B[39m, operation_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, request\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n",
       "\u001B[1;32m     99\u001B[0m     \u001B[38;5;66;03m# This is typically hooked up to the \"request-created\" event\u001B[39;00m\n",
       "\u001B[1;32m    100\u001B[0m     \u001B[38;5;66;03m# from a client's event emitter.  When a new request is created\u001B[39;00m\n",
       "\u001B[1;32m    101\u001B[0m     \u001B[38;5;66;03m# this method is invoked to sign the request.\u001B[39;00m\n",
       "\u001B[1;32m    102\u001B[0m     \u001B[38;5;66;03m# Don't call this method directly.\u001B[39;00m\n",
       "\u001B[0;32m--> 103\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msign\u001B[49m\u001B[43m(\u001B[49m\u001B[43moperation_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/botocore/signers.py:187\u001B[0m, in \u001B[0;36mRequestSigner.sign\u001B[0;34m(self, operation_name, request, region_name, signing_type, expires_in, signing_name)\u001B[0m\n",
       "\u001B[1;32m    184\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    185\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[0;32m--> 187\u001B[0m \u001B[43mauth\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_auth\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/botocore/auth.py:407\u001B[0m, in \u001B[0;36mSigV4Auth.add_auth\u001B[0;34m(self, request)\u001B[0m\n",
       "\u001B[1;32m    405\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madd_auth\u001B[39m(\u001B[38;5;28mself\u001B[39m, request):\n",
       "\u001B[1;32m    406\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcredentials \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m--> 407\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m NoCredentialsError()\n",
       "\u001B[1;32m    408\u001B[0m     datetime_now \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mdatetime\u001B[38;5;241m.\u001B[39mutcnow()\n",
       "\u001B[1;32m    409\u001B[0m     request\u001B[38;5;241m.\u001B[39mcontext[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtimestamp\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m datetime_now\u001B[38;5;241m.\u001B[39mstrftime(SIGV4_TIMESTAMP)\n",
       "\n",
       "\u001B[0;31mNoCredentialsError\u001B[0m: Unable to locate credentials"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNoCredentialsError\u001B[0m                        Traceback (most recent call last)\nFile \u001B[0;32m<command-924525323806191>, line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m s3_client \u001B[38;5;241m=\u001B[39m boto3\u001B[38;5;241m.\u001B[39mclient(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms3\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Obtenha a localização do bucket\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m bucket_location \u001B[38;5;241m=\u001B[39m s3_client\u001B[38;5;241m.\u001B[39mget_bucket_location(Bucket\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mailos-teste\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Extraia a região do bucket\u001B[39;00m\n\u001B[1;32m     10\u001B[0m region \u001B[38;5;241m=\u001B[39m bucket_location[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLocationConstraint\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/botocore/client.py:508\u001B[0m, in \u001B[0;36mClientCreator._create_api_method.<locals>._api_call\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    504\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    505\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpy_operation_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m() only accepts keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    506\u001B[0m     )\n\u001B[1;32m    507\u001B[0m \u001B[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001B[39;00m\n\u001B[0;32m--> 508\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_api_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43moperation_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/botocore/client.py:898\u001B[0m, in \u001B[0;36mBaseClient._make_api_call\u001B[0;34m(self, operation_name, api_params)\u001B[0m\n\u001B[1;32m    896\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    897\u001B[0m     apply_request_checksum(request_dict)\n\u001B[0;32m--> 898\u001B[0m     http, parsed_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    899\u001B[0m \u001B[43m        \u001B[49m\u001B[43moperation_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest_context\u001B[49m\n\u001B[1;32m    900\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    902\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmeta\u001B[38;5;241m.\u001B[39mevents\u001B[38;5;241m.\u001B[39memit(\n\u001B[1;32m    903\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mafter-call.\u001B[39m\u001B[38;5;132;01m{service_id}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{operation_name}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    904\u001B[0m         service_id\u001B[38;5;241m=\u001B[39mservice_id, operation_name\u001B[38;5;241m=\u001B[39moperation_name\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    909\u001B[0m     context\u001B[38;5;241m=\u001B[39mrequest_context,\n\u001B[1;32m    910\u001B[0m )\n\u001B[1;32m    912\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m300\u001B[39m:\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/botocore/client.py:921\u001B[0m, in \u001B[0;36mBaseClient._make_request\u001B[0;34m(self, operation_model, request_dict, request_context)\u001B[0m\n\u001B[1;32m    919\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_make_request\u001B[39m(\u001B[38;5;28mself\u001B[39m, operation_model, request_dict, request_context):\n\u001B[1;32m    920\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 921\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_endpoint\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43moperation_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    922\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    923\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmeta\u001B[38;5;241m.\u001B[39mevents\u001B[38;5;241m.\u001B[39memit(\n\u001B[1;32m    924\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mafter-call-error.\u001B[39m\u001B[38;5;132;01m{service_id}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{operation_name}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    925\u001B[0m                 service_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_service_model\u001B[38;5;241m.\u001B[39mservice_id\u001B[38;5;241m.\u001B[39mhyphenize(),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    929\u001B[0m             context\u001B[38;5;241m=\u001B[39mrequest_context,\n\u001B[1;32m    930\u001B[0m         )\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/botocore/endpoint.py:119\u001B[0m, in \u001B[0;36mEndpoint.make_request\u001B[0;34m(self, operation_model, request_dict)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmake_request\u001B[39m(\u001B[38;5;28mself\u001B[39m, operation_model, request_dict):\n\u001B[1;32m    114\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\n\u001B[1;32m    115\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMaking request for \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m with params: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    116\u001B[0m         operation_model,\n\u001B[1;32m    117\u001B[0m         request_dict,\n\u001B[1;32m    118\u001B[0m     )\n\u001B[0;32m--> 119\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moperation_model\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/botocore/endpoint.py:198\u001B[0m, in \u001B[0;36mEndpoint._send_request\u001B[0;34m(self, request_dict, operation_model)\u001B[0m\n\u001B[1;32m    196\u001B[0m context \u001B[38;5;241m=\u001B[39m request_dict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontext\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_retries_context(context, attempts)\n\u001B[0;32m--> 198\u001B[0m request \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moperation_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    199\u001B[0m success_response, exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_response(\n\u001B[1;32m    200\u001B[0m     request, operation_model, context\n\u001B[1;32m    201\u001B[0m )\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_needs_retry(\n\u001B[1;32m    203\u001B[0m     attempts,\n\u001B[1;32m    204\u001B[0m     operation_model,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    207\u001B[0m     exception,\n\u001B[1;32m    208\u001B[0m ):\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/botocore/endpoint.py:134\u001B[0m, in \u001B[0;36mEndpoint.create_request\u001B[0;34m(self, params, operation_model)\u001B[0m\n\u001B[1;32m    130\u001B[0m     service_id \u001B[38;5;241m=\u001B[39m operation_model\u001B[38;5;241m.\u001B[39mservice_model\u001B[38;5;241m.\u001B[39mservice_id\u001B[38;5;241m.\u001B[39mhyphenize()\n\u001B[1;32m    131\u001B[0m     event_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrequest-created.\u001B[39m\u001B[38;5;132;01m{service_id}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{op_name}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    132\u001B[0m         service_id\u001B[38;5;241m=\u001B[39mservice_id, op_name\u001B[38;5;241m=\u001B[39moperation_model\u001B[38;5;241m.\u001B[39mname\n\u001B[1;32m    133\u001B[0m     )\n\u001B[0;32m--> 134\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_event_emitter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43memit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    135\u001B[0m \u001B[43m        \u001B[49m\u001B[43mevent_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    136\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    137\u001B[0m \u001B[43m        \u001B[49m\u001B[43moperation_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moperation_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    138\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    139\u001B[0m prepared_request \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_request(request)\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m prepared_request\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/botocore/hooks.py:412\u001B[0m, in \u001B[0;36mEventAliaser.emit\u001B[0;34m(self, event_name, **kwargs)\u001B[0m\n\u001B[1;32m    410\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21memit\u001B[39m(\u001B[38;5;28mself\u001B[39m, event_name, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    411\u001B[0m     aliased_event_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_alias_event_name(event_name)\n\u001B[0;32m--> 412\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_emitter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43memit\u001B[49m\u001B[43m(\u001B[49m\u001B[43maliased_event_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/botocore/hooks.py:256\u001B[0m, in \u001B[0;36mHierarchicalEmitter.emit\u001B[0;34m(self, event_name, **kwargs)\u001B[0m\n\u001B[1;32m    245\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21memit\u001B[39m(\u001B[38;5;28mself\u001B[39m, event_name, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    246\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    247\u001B[0m \u001B[38;5;124;03m    Emit an event by name with arguments passed as keyword args.\u001B[39;00m\n\u001B[1;32m    248\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    254\u001B[0m \u001B[38;5;124;03m             handlers.\u001B[39;00m\n\u001B[1;32m    255\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 256\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_emit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevent_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/botocore/hooks.py:239\u001B[0m, in \u001B[0;36mHierarchicalEmitter._emit\u001B[0;34m(self, event_name, kwargs, stop_on_response)\u001B[0m\n\u001B[1;32m    237\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m handler \u001B[38;5;129;01min\u001B[39;00m handlers_to_call:\n\u001B[1;32m    238\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEvent \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m: calling handler \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m, event_name, handler)\n\u001B[0;32m--> 239\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mhandler\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    240\u001B[0m     responses\u001B[38;5;241m.\u001B[39mappend((handler, response))\n\u001B[1;32m    241\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stop_on_response \u001B[38;5;129;01mand\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/botocore/signers.py:103\u001B[0m, in \u001B[0;36mRequestSigner.handler\u001B[0;34m(self, operation_name, request, **kwargs)\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhandler\u001B[39m(\u001B[38;5;28mself\u001B[39m, operation_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, request\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     99\u001B[0m     \u001B[38;5;66;03m# This is typically hooked up to the \"request-created\" event\u001B[39;00m\n\u001B[1;32m    100\u001B[0m     \u001B[38;5;66;03m# from a client's event emitter.  When a new request is created\u001B[39;00m\n\u001B[1;32m    101\u001B[0m     \u001B[38;5;66;03m# this method is invoked to sign the request.\u001B[39;00m\n\u001B[1;32m    102\u001B[0m     \u001B[38;5;66;03m# Don't call this method directly.\u001B[39;00m\n\u001B[0;32m--> 103\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msign\u001B[49m\u001B[43m(\u001B[49m\u001B[43moperation_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/botocore/signers.py:187\u001B[0m, in \u001B[0;36mRequestSigner.sign\u001B[0;34m(self, operation_name, request, region_name, signing_type, expires_in, signing_name)\u001B[0m\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    185\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m--> 187\u001B[0m \u001B[43mauth\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_auth\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/botocore/auth.py:407\u001B[0m, in \u001B[0;36mSigV4Auth.add_auth\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    405\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madd_auth\u001B[39m(\u001B[38;5;28mself\u001B[39m, request):\n\u001B[1;32m    406\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcredentials \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 407\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m NoCredentialsError()\n\u001B[1;32m    408\u001B[0m     datetime_now \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mdatetime\u001B[38;5;241m.\u001B[39mutcnow()\n\u001B[1;32m    409\u001B[0m     request\u001B[38;5;241m.\u001B[39mcontext[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtimestamp\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m datetime_now\u001B[38;5;241m.\u001B[39mstrftime(SIGV4_TIMESTAMP)\n\n\u001B[0;31mNoCredentialsError\u001B[0m: Unable to locate credentials",
       "errorSummary": "<span class='ansi-red-fg'>NoCredentialsError</span>: Unable to locate credentials",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Crie uma instância do cliente S3\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Obtenha a localização do bucket\n",
    "bucket_location = s3_client.get_bucket_location(Bucket='ailos-teste')\n",
    "\n",
    "# Extraia a região do bucket\n",
    "region = bucket_location['LocationConstraint']\n",
    "\n",
    "# Construa o endpoint do S3\n",
    "endpoint = f\"s3.{region}.amazonaws.com\"\n",
    "\n",
    "print(endpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "859351ce-9e55-4cdc-a082-f096277365ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: boto3 in /databricks/python3/lib/python3.10/site-packages (1.24.28)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /databricks/python3/lib/python3.10/site-packages (from boto3) (0.6.0)\nRequirement already satisfied: botocore<1.28.0,>=1.27.28 in /databricks/python3/lib/python3.10/site-packages (from boto3) (1.27.28)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.10/site-packages (from boto3) (0.10.0)\nRequirement already satisfied: urllib3<1.27,>=1.25.4 in /databricks/python3/lib/python3.10/site-packages (from botocore<1.28.0,>=1.27.28->boto3) (1.26.11)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /databricks/python3/lib/python3.10/site-packages (from botocore<1.28.0,>=1.27.28->boto3) (2.8.2)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.28->boto3) (1.16.0)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa05527c-1ac4-4c01-b8a9-441e02956ef6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-924525323806181>, line 13\u001B[0m\n",
       "\u001B[1;32m     10\u001B[0m         registrar_log(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mErro\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mErro ao listar arquivos no Bucket S3: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     11\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m []\n",
       "\u001B[0;32m---> 13\u001B[0m arquivos_cext \u001B[38;5;241m=\u001B[39m ler_arquivos_cext(\u001B[43mmount_path\u001B[49m)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'mount_path' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-924525323806181>, line 13\u001B[0m\n\u001B[1;32m     10\u001B[0m         registrar_log(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mErro\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mErro ao listar arquivos no Bucket S3: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     11\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m []\n\u001B[0;32m---> 13\u001B[0m arquivos_cext \u001B[38;5;241m=\u001B[39m ler_arquivos_cext(\u001B[43mmount_path\u001B[49m)\n\n\u001B[0;31mNameError\u001B[0m: name 'mount_path' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'mount_path' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2.\tMáscara para identificar arquivo CEXT, validar se o nome do arquivo obedece a regra abaixo:\n",
    "##  a.\tCEXT_756*.CCB\n",
    "\n",
    "def ler_arquivos_cext(mount_path):\n",
    "    try:\n",
    "        files = dbutils.fs.ls(mount_path)\n",
    "        arquivos_cext = [file.path for file in files if file.name.startswith(\"CEXT_756\")]\n",
    "        return arquivos_cext\n",
    "    except Exception as e:\n",
    "        registrar_log('Erro', f\"Erro ao listar arquivos no Bucket S3: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "arquivos_cext = ler_arquivos_cext(mount_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea81dd6b-7883-4d45-ab3c-bb3e599d6953",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A estrutura do arquivo é válida.\n"
     ]
    }
   ],
   "source": [
    "# 3. Validar estrutura do arquivo\n",
    "##  a.\tPrimeira linha é cabeçalho e deve começar com = \"CEXT0\"\n",
    "##  b.\tÚltima linha é o trailer e deve começar com = \"CEXT9\"\n",
    "\n",
    "# Caminho do arquivo\n",
    "caminho_arquivo = \"/mnt/s3-ailos-teste/CEXT_7562011_20240125_0002504.CCB\"\n",
    "\n",
    "# Ler o conteúdo do arquivo como um DataFrame\n",
    "df = spark.read.text(caminho_arquivo)\n",
    "\n",
    "# Extrair o conteúdo como uma lista de strings\n",
    "linhas = [row.value for row in df.collect()]\n",
    "\n",
    "# Validar a estrutura do arquivo\n",
    "if linhas[0].startswith(\"CEXT0\") and linhas[-1].startswith(\"CEXT9\"):\n",
    "    print(\"A estrutura do arquivo é válida.\")\n",
    "else:\n",
    "    print(\"A estrutura do arquivo não é válida.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aa481be-49e1-4e22-afcf-9d293fa86ffe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Diretório bronze\n",
    "bronze_path = mount_path + \"/bronze\"\n",
    "\n",
    "# Criar a subpasta bronze\n",
    "dbutils.fs.mkdirs(bronze_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f4c7660-67fa-40ff-abb0-4b29e856d4b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Diretório log\n",
    "log_path = bronze_path + \"/log\"\n",
    "\n",
    "# Criar a subpasta log\n",
    "dbutils.fs.mkdirs(log_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97d05b27-7f4e-494c-b959-b083747359c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "\n",
    "# Inicializar a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Criar Tabela de Log\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Definir o esquema da tabela de log\n",
    "log_schema = StructType([\n",
    "    StructField(\"job_id\", StringType(), False),\n",
    "    StructField(\"job_name\", StringType(), False),\n",
    "    StructField(\"start_time\", TimestampType(), False),\n",
    "    StructField(\"end_time\", TimestampType(), True),\n",
    "    StructField(\"status\", StringType(), False),\n",
    "    StructField(\"error_message\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Caminho para a pasta da camada bronze\n",
    "bronze_path = mount_path + \"/bronze\"\n",
    "\n",
    "# Caminho para a pasta de log\n",
    "log_path = bronze_path + \"/log\"\n",
    "\n",
    "# Criar a subpasta de log\n",
    "dbutils.fs.mkdirs(log_path)\n",
    "\n",
    "# Caminho para o arquivo Delta Lake da tabela de log\n",
    "log_delta_path = log_path + \"/log_table.delta\"\n",
    "\n",
    "# Criar a tabela Delta Lake na camada bronze\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS delta.`{log_delta_path}`\n",
    "    USING delta\n",
    "    LOCATION '{log_delta_path}'\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3563524d-8220-43a6-8add-962841a12520",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|               value|\n+--------------------+\n|CEXT0756202401250...|\n|75600051510700443...|\n|75600051510700443...|\n|75600051510700443...|\n|75600051510704320...|\n|75600051510704320...|\n|75600051510704320...|\n|75600051510704320...|\n|75600051519402304...|\n|75600051519402304...|\n|75600051510700443...|\n|CEXT9030813900000...|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Importar as bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Criar uma sessão Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Leitura de Arquivo CCB\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Caminho do arquivo\n",
    "caminho_arquivo = \"/mnt/s3-ailos-teste/CEXT_7562011_20240125_0002504.CCB\"\n",
    "\n",
    "# Ler o conteúdo do arquivo como um DataFrame\n",
    "df = spark.read.text(caminho_arquivo)\n",
    "\n",
    "# Mostrar as primeiras linhas do DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39b7fd35-43c2-4803-ba0a-72d040800834",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+----------------+----------+------------+-----------+----------+----------+--------------+--------+-------------------+-------------------+--------------------+-----------+------------+\n|ID_TODOS|            IDCARTAO|   IDCARTAO_PROPOSTA|      IDCONTA_CARTAO|        NRCARTAO|DTVALIDADE|NRDIA_DEBITO|TPPAGAMENTO|CDSITUACAO| DTENTREGA|DTCANCELAMENTO|CDMOTIVO|CDOPERADOR_INCLUSAO|         DHREGISTRO|CDOPERADOR_ALTERACAO|DHALTERACAO|IDCOMPONENTE|\n+--------+--------------------+--------------------+--------------------+----------------+----------+------------+-----------+----------+----------+--------------+--------+-------------------+-------------------+--------------------+-----------+------------+\n|      A1|FF95CEC4CBF82F48E...|FF95CEC4CBF72F48E...|FF95CEC4C8792F48E...|5151070044387015|2027-06-30|          10|          1|         3|2023-06-15|          NULL|    NULL|                  1|2023-06-15 10:32:38|                NULL|       NULL|        NULL|\n|      A2|FF95CEC4CBFC2F48E...|FF95CEC4CBFB2F48E...|FF95CEC4C87B2F48E...|5151070044381239|2027-06-30|          10|          1|         3|2023-06-15|          NULL|    NULL|                  1|2023-06-15 10:32:38|                NULL|       NULL|        NULL|\n+--------+--------------------+--------------------+--------------------+----------------+----------+------------+-----------+----------+----------+--------------+--------+-------------------+-------------------+--------------------+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Importar as bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Inicializar a sessão do Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Criar_TB_CARTAO\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Definir o esquema da tabela TB_CARTAO\n",
    "schema = StructType([\n",
    "    StructField(\"ID_TODOS\", StringType(), False),\n",
    "    StructField(\"IDCARTAO\", StringType(), False),\n",
    "    StructField(\"IDCARTAO_PROPOSTA\", StringType(), False),\n",
    "    StructField(\"IDCONTA_CARTAO\", StringType(), False),\n",
    "    StructField(\"NRCARTAO\", StringType(), False),\n",
    "    StructField(\"DTVALIDADE\", StringType(), True),\n",
    "    StructField(\"NRDIA_DEBITO\", IntegerType(), True),\n",
    "    StructField(\"TPPAGAMENTO\", IntegerType(), True),\n",
    "    StructField(\"CDSITUACAO\", IntegerType(), False),\n",
    "    StructField(\"DTENTREGA\", StringType(), True),\n",
    "    StructField(\"DTCANCELAMENTO\", StringType(), True),\n",
    "    StructField(\"CDMOTIVO\", IntegerType(), True),\n",
    "    StructField(\"CDOPERADOR_INCLUSAO\", StringType(), False),\n",
    "    StructField(\"DHREGISTRO\", StringType(), False),\n",
    "    StructField(\"CDOPERADOR_ALTERACAO\", StringType(), True),\n",
    "    StructField(\"DHALTERACAO\", StringType(), True),\n",
    "    StructField(\"IDCOMPONENTE\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Dados fictícios\n",
    "data = [\n",
    "    ('A1','FF95CEC4CBF82F48E0530100007FBDE5', 'FF95CEC4CBF72F48E0530100007FBDE5', 'FF95CEC4C8792F48E0530100007FBDE5', '5151070044387015', '2027-06-30', 10, 1, 3, '2023-06-15', None, None, '1', '2023-06-15 10:32:38', None, None, None),\n",
    "    ('A2','FF95CEC4CBFC2F48E0530100007FBDE5', 'FF95CEC4CBFB2F48E0530100007FBDE5', 'FF95CEC4C87B2F48E0530100007FBDE5', '5151070044381239', '2027-06-30', 10, 1, 3, '2023-06-15', None, None, '1', '2023-06-15 10:32:38', None, None, None)\n",
    "]\n",
    "\n",
    "# Criar DataFrame com os dados fictícios e o esquema definido\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Especificar o caminho onde os dados serão armazenados\n",
    "caminho_dados = \"/mnt/s3-ailos-teste/TB_CARTAO\"\n",
    "\n",
    "# Escrever o DataFrame para o Delta Lake\n",
    "df.write.format(\"delta\").save(caminho_dados)\n",
    "\n",
    "# Exibir os dados\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f209738-2bc8-474e-9c0e-7efa42a57ffb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------+-----------+--------------------+-------------------+----------+\n|ID_TODOS|  IDARQUIVO_CONTROLE|NRARQUIVO|DTALTERACAO|NRSEQUENCIAL_ARQUIVO|DSDIRETORIO_ARQUIVO|DHREGISTRO|\n+--------+--------------------+---------+-----------+--------------------+-------------------+----------+\n|      A1|FF95CEC4CE732F48E...|        2| 2024-05-07|                3731|              /temp|2024-05-07|\n|      A2|FF95CEC4D31A2F48E...|        3| 2024-05-07|                3131|            /recebe|2024-05-07|\n+--------+--------------------+---------+-----------+--------------------+-------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Importar as bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "# Definir o esquema da tabela TB_ARQUIVO_CONTROLE\n",
    "schema_arquivo_controle = StructType([\n",
    "    StructField(\"ID_TODOS\", StringType(), False),\n",
    "    StructField(\"IDARQUIVO_CONTROLE\", StringType(), False),\n",
    "    StructField(\"NRARQUIVO\", IntegerType(), False),\n",
    "    StructField(\"DTALTERACAO\", StringType(), True),\n",
    "    StructField(\"NRSEQUENCIAL_ARQUIVO\", IntegerType(), True),\n",
    "    StructField(\"DSDIRETORIO_ARQUIVO\", StringType(), True),\n",
    "    StructField(\"DHREGISTRO\", StringType(), False)\n",
    "])\n",
    "\n",
    "# Dados fictícios\n",
    "data_arquivo_controle = [\n",
    "    ('A1','FF95CEC4CE732F48E0530100007FBDE5', 2, '2024-05-07', 3731, '/temp', '2024-05-07'),\n",
    "    ('A2','FF95CEC4D31A2F48E0530100007FBDE5', 3, '2024-05-07', 3131, '/recebe', '2024-05-07')\n",
    "]\n",
    "\n",
    "# Criar DataFrame com os dados fictícios e o esquema definido\n",
    "df_arquivo_controle = spark.createDataFrame(data_arquivo_controle, schema_arquivo_controle)\n",
    "\n",
    "# Especificar o caminho onde os dados serão armazenados\n",
    "caminho_dados_arquivo_controle = \"/mnt/s3-ailos-teste/TB_ARQUIVO_CONTROLE\"\n",
    "\n",
    "# Escrever o DataFrame para o Delta Lake\n",
    "df_arquivo_controle.write.format(\"delta\").save(caminho_dados_arquivo_controle)\n",
    "\n",
    "# Exibir os dados\n",
    "df_arquivo_controle.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfdbefe2-6026-4b8e-81e0-59804b9bcace",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+------------+----------+-----------------+--------------+------------------+-------+----------+\n|ID_TODOS|           IDARQUIVO|  IDARQUIVO_CONTROLE|   NMARQUIVO| DTARQUIVO|DTINICIO_PROCESSO|DTFIM_PROCESSO|QTREGISTRO_ARQUIVO|VLTOTAL|DHREGISTRO|\n+--------+--------------------+--------------------+------------+----------+-----------------+--------------+------------------+-------+----------+\n|      A1|FF95CEC4CE732F48E...|FF95CEC4CE732F48E...|arquivo1.txt|2024-05-07|             NULL|          NULL|              NULL|   NULL|2024-05-07|\n|      A2|FF95CEC4D31A2F48E...|FF95CEC4D31A2F48E...|arquivo2.txt|2024-05-07|             NULL|          NULL|              NULL|   NULL|2024-05-07|\n+--------+--------------------+--------------------+------------+----------+-----------------+--------------+------------------+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Importar as bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "# Definir o esquema da tabela TB_ARQUIVO\n",
    "schema_arquivo = StructType([\n",
    "    StructField(\"ID_TODOS\", StringType(), False),\n",
    "    StructField(\"IDARQUIVO\", StringType(), False),\n",
    "    StructField(\"IDARQUIVO_CONTROLE\", StringType(), False),\n",
    "    StructField(\"NMARQUIVO\", StringType(), False),\n",
    "    StructField(\"DTARQUIVO\", StringType(), True),\n",
    "    StructField(\"DTINICIO_PROCESSO\", StringType(), True),\n",
    "    StructField(\"DTFIM_PROCESSO\", StringType(), True),\n",
    "    StructField(\"QTREGISTRO_ARQUIVO\", IntegerType(), True),\n",
    "    StructField(\"VLTOTAL\", StringType(), True),\n",
    "    StructField(\"DHREGISTRO\", StringType(), False)\n",
    "])\n",
    "\n",
    "# Dados fictícios\n",
    "data_arquivo = [\n",
    "    ('A1','FF95CEC4CE732F48E0530100007FBDE5', 'FF95CEC4CE732F48E0530100007FBDE5', 'arquivo1.txt', '2024-05-07', None, None, None, None, '2024-05-07'),\n",
    "    ('A2','FF95CEC4D31A2F48E0530100007FBDE5', 'FF95CEC4D31A2F48E0530100007FBDE5', 'arquivo2.txt', '2024-05-07', None, None, None, None, '2024-05-07')\n",
    "]\n",
    "\n",
    "# Criar DataFrame com os dados fictícios e o esquema definido\n",
    "df_arquivo = spark.createDataFrame(data_arquivo, schema_arquivo)\n",
    "\n",
    "# Especificar o caminho onde os dados serão armazenados\n",
    "caminho_dados_arquivo = \"/mnt/s3-ailos-teste/TB_ARQUIVO\"\n",
    "\n",
    "# Escrever o DataFrame para o Delta Lake\n",
    "df_arquivo.write.format(\"delta\").save(caminho_dados_arquivo)\n",
    "\n",
    "# Exibir os dados\n",
    "df_arquivo.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a56f27f0-0aca-4047-8e50-7df174af2308",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+-------+----------+----------+----------+----------+\n|ID_TODOS|     IDARQUIVO_LINHA|           IDARQUIVO|NRLINHA|DSCONTEUDO|DTPROCESSO|CDSITUACAO|DHREGISTRO|\n+--------+--------------------+--------------------+-------+----------+----------+----------+----------+\n|      A1|FF95CEC4CE732F48E...|FF95CEC4CE732F48E...|      1| conteudo1|2024-05-07|         1|2024-05-07|\n|      A2|FF95CEC4D31A2F48E...|FF95CEC4D31A2F48E...|      2| conteudo2|2024-05-07|         1|2024-05-07|\n+--------+--------------------+--------------------+-------+----------+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Definir o esquema da tabela TB_ARQUIVO_LINHA\n",
    "schema_arquivo_linha = StructType([\n",
    "    StructField(\"ID_TODOS\", StringType(), False),\n",
    "    StructField(\"IDARQUIVO_LINHA\", StringType(), False),\n",
    "    StructField(\"IDARQUIVO\", StringType(), False),\n",
    "    StructField(\"NRLINHA\", IntegerType(), False),\n",
    "    StructField(\"DSCONTEUDO\", StringType(), False),\n",
    "    StructField(\"DTPROCESSO\", StringType(), True),\n",
    "    StructField(\"CDSITUACAO\", IntegerType(), True),\n",
    "    StructField(\"DHREGISTRO\", StringType(), False)\n",
    "])\n",
    "\n",
    "# Dados fictícios\n",
    "data_arquivo_linha = [\n",
    "    ('A1','FF95CEC4CE732F48E0530100007FBDE5', 'FF95CEC4CE732F48E0530100007FBDE5', 1, 'conteudo1', '2024-05-07', 1, '2024-05-07'),\n",
    "    ('A2','FF95CEC4D31A2F48E0530100007FBDE5', 'FF95CEC4D31A2F48E0530100007FBDE5', 2, 'conteudo2', '2024-05-07', 1, '2024-05-07')\n",
    "]\n",
    "\n",
    "# Criar DataFrame com os dados fictícios e o esquema definido\n",
    "df_arquivo_linha = spark.createDataFrame(data_arquivo_linha, schema_arquivo_linha)\n",
    "\n",
    "# Especificar o caminho onde os dados serão armazenados\n",
    "caminho_dados_arquivo_linha = \"/mnt/s3-ailos-teste/TB_ARQUIVO_LINHA\"\n",
    "\n",
    "# Escrever o DataFrame para o Delta Lake\n",
    "df_arquivo_linha.write.format(\"delta\").save(caminho_dados_arquivo_linha)\n",
    "\n",
    "# Exibir os dados\n",
    "df_arquivo_linha.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5d690e8-adc9-441a-ae03-c341183c56a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|value                                                                                                                                                                                                                                                                                                      |\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|CEXT0756202401250002404                                                                                                                                                                                                                                                                                    |\n|7560005151070044381239   2D0100612202306122023        0000000680000000000000334806ALUMINIO SAO JOSE      SAO JOSE      BRA0000098600000006800                       003232000000000027               21565812061325250000000010010263013889733        00000000200                                          |\n|7560005151070044381239   2D0100612202306122023        0000035521300000000000308441PERFIL SUL             SAO JOSE      BRA0000098600000355213                       003232000000000027               3751281206173213000000001001820063N5U5M1V        00020000200                                          |\n|7560005151070044381239   2D0100612202306122023        0000000089000000000000000490CHOCOILHA ALIMENTOS E  FLORIANOPOLIS BRA0000098600000000890                       003232000000000035               80160412061411360000000010011535200146008        00000000200                                          |\n|7560005151070432093151   2D0100612202306122023        0000000120000000000000190346MP EMPORIUMDOJAIAVENI OSASCO        BRA 0000098600000001200                       003232000000000213               27233912062052540000000010026062591719425        00000000200                                          |\n|7560005151070432093151   2D0100612202306122023        0000000275000000000000112538ESTEFANO XAUTZ         FLORIANOPOLIS BRA0000098600000002750                       003232000000000213               38756412061438590000000010012415901149275        00000000200                                          |\n|7560005151070432093151   2D0100612202306122023        0000000120000000000000448301MP EMPORIUMDOJAI      SAO JOSE      BRA 0000098600000001200                       003232000000000213               04126712062007530000000010023809091719425        00000000200                                          |\n|7560005151070432093151   2D0100612202306122023        0000000120000000000000454287MP EMPORIUMDOJAI      SAO JOSE      BRA 0000098600000001200                       003232000000000213               06192112062009080000000010023868891719425        00000000200                                          |\n|7560005151940230409696   2D0100612202306122023        0000002119800000000000241800LEMAR AUTO POSTO LTDA  Porto Belo    BRA0000098600000021198                       003232000000000299               782886120617130900000000100176593DI000002        00000000200                                          |\n|7560005151940230409696   2D0100612202306122023        0000001375000000000000216597TAMOYO COMERCIO DE FER PORTO BELO    BRA0000098600000013750                       003232000000000299               50019912061916490000000010021777000000002        00000000200                                          |\n|7560005151070044387015   2D0100612202306122023        0000001000000000000000469942PAGNucleoEspiritaNos  SAO JOSE      BRA 0000098600000010000                       003232000000000345               870679120611310700000000100073573AY561925        00000000200                                          |\n|CEXT90308139000000002609489793                                                                                                                                                                                                                                                                             |\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializar a sessão Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Análise de Arquivo CCB\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Caminho do arquivo .CCB\n",
    "caminho_arquivo = \"/mnt/s3-ailos-teste/CEXT_7562011_20240125_0002504.CCB\"\n",
    "\n",
    "# Ler o arquivo .CCB como um DataFrame\n",
    "df = spark.read.text(caminho_arquivo)\n",
    "\n",
    "# Mostrar as primeiras linhas do DataFrame\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f6ce577-ba33-428e-9760-35204ca24a7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n|   numero_cartao|\n+----------------+\n|5151070044381239|\n|5151070044381239|\n|5151070044381239|\n|5151070432093151|\n|5151070432093151|\n|5151070432093151|\n|5151070432093151|\n|5151940230409696|\n|5151940230409696|\n|5151070044387015|\n+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Caminho do arquivo CCB\n",
    "file_path = \"/mnt/s3-ailos-teste/CEXT_7562011_20240125_0002504.CCB\"\n",
    "\n",
    "# Ler o arquivo CCB\n",
    "ccb_df = spark.read.text(file_path)\n",
    "\n",
    "# Remover a primeira e última linha do DataFrame\n",
    "ccb_df = ccb_df.filter(~(col(\"value\").startswith(\"CEXT\") | col(\"value\").endswith(\"CEXT90308139000000002609489793\")))\n",
    "\n",
    "# Definir uma função para extrair o número do cartão de uma linha do arquivo CCB\n",
    "def extrair_numero_cartao(linha):\n",
    "    return linha[6:25].strip()\n",
    "\n",
    "# Aplicar a função a cada linha do DataFrame para extrair os números de cartão\n",
    "numeros_cartao = ccb_df.rdd.map(lambda x: extrair_numero_cartao(x[0])).collect()\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Converter a lista de números de cartão em linhas do DataFrame\n",
    "numeros_cartao_rows = [Row(numero_cartao=num) for num in numeros_cartao]\n",
    "\n",
    "# Criar DataFrame a partir das linhas\n",
    "numeros_cartao_df = spark.createDataFrame(numeros_cartao_rows)\n",
    "\n",
    "# Mostrar os números de cartão em coluna\n",
    "numeros_cartao_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b58b614f-ed05-4f7c-ac74-b1508d0f296b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n|        NRCARTAO|\n+----------------+\n|5151070044387015|\n|5151070044381239|\n+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Caminho dos dados da tabela TB_CARTAO\n",
    "caminho_dados_tb_cartao = \"/mnt/s3-ailos-teste/TB_CARTAO\"\n",
    "\n",
    "# Carregar a tabela TB_CARTAO do Delta Lake\n",
    "tb_cartao_df = spark.read.format(\"delta\").load(caminho_dados_tb_cartao)\n",
    "\n",
    "# Selecionar a coluna NRCARTAO\n",
    "nrcartao_column_df = tb_cartao_df.select(\"NRCARTAO\")\n",
    "\n",
    "# Mostrar os números de cartão em coluna\n",
    "nrcartao_column_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e42f91be-472c-4554-879e-304464af2200",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|value                                                                                                                                                                                                                                                                                                      |\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|7560005151070044387015   2D0100612202306122023        0000001000000000000000469942PAGNucleoEspiritaNos  SAO JOSE      BRA 0000098600000010000                       003232000000000345               870679120611310700000000100073573AY561925        00000000200                                          |\n|7560005151070044381239   2D0100612202306122023        0000000680000000000000334806ALUMINIO SAO JOSE      SAO JOSE      BRA0000098600000006800                       003232000000000027               21565812061325250000000010010263013889733        00000000200                                          |\n|7560005151070044381239   2D0100612202306122023        0000035521300000000000308441PERFIL SUL             SAO JOSE      BRA0000098600000355213                       003232000000000027               3751281206173213000000001001820063N5U5M1V        00020000200                                          |\n|7560005151070044381239   2D0100612202306122023        0000000089000000000000000490CHOCOILHA ALIMENTOS E  FLORIANOPOLIS BRA0000098600000000890                       003232000000000035               80160412061411360000000010011535200146008        00000000200                                          |\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Inicializar a sessão do Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Validacao de Cartoes\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Caminho da tabela Delta TB_CARTAO\n",
    "caminho_tb_cartao = \"/mnt/s3-ailos-teste/TB_CARTAO\"\n",
    "\n",
    "# Carregar a tabela Delta TB_CARTAO\n",
    "tb_cartao_df = spark.read.format(\"delta\").load(caminho_tb_cartao)\n",
    "\n",
    "# Caminho do arquivo CCB\n",
    "caminho_ccb = \"/mnt/s3-ailos-teste/CEXT_7562011_20240125_0002504.CCB\"\n",
    "\n",
    "# Carregar o DataFrame do arquivo CCB\n",
    "ccb_df = spark.read.text(caminho_ccb)\n",
    "\n",
    "# Remover a primeira e última linha do DataFrame\n",
    "ccb_df = ccb_df.filter(~(col(\"value\").startswith(\"CEXT\") | col(\"value\").endswith(\"CEXT90308139000000002609489793\")))\n",
    "\n",
    "# Definir uma função para extrair o número do cartão de uma linha do arquivo CCB\n",
    "def extrair_numero_cartao(linha):\n",
    "    return linha[6:25].strip()\n",
    "\n",
    "# Aplicar a função a cada linha do DataFrame para extrair os números de cartão\n",
    "numeros_cartao_ailos = set(ccb_df.rdd.map(lambda x: extrair_numero_cartao(x[0])).collect())\n",
    "\n",
    "# Criar um DataFrame vazio para armazenar as linhas correspondentes do arquivo CCB\n",
    "silver_df = spark.createDataFrame([], ccb_df.schema)\n",
    "\n",
    "# Iterar sobre os números de cartão da tabela TB_CARTAO\n",
    "for numero_cartao in tb_cartao_df.select(\"NRCARTAO\").distinct().collect():\n",
    "    numero_cartao = numero_cartao[\"NRCARTAO\"]\n",
    "    # Verificar se o número de cartão está presente nos números de cartão do arquivo CCB\n",
    "    if numero_cartao in numeros_cartao_ailos:\n",
    "        # Adicionar a linha correspondente do arquivo CCB à planilha Silver\n",
    "        linha_correspondente = ccb_df.filter(ccb_df.value.contains(numero_cartao))\n",
    "        silver_df = silver_df.union(linha_correspondente)\n",
    "\n",
    "# Exibir todas as linhas do DataFrame silver_df\n",
    "silver_df.show(truncate=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19e05cf6-9ce4-4963-963a-76326708e410",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A estrutura do arquivo é válida.\n+-----+------+----------------+--------------------+---------------+\n|linha|codigo|   numero_cartao|         inf_arquivo|Valor_transacao|\n+-----+------+----------------+--------------------+---------------+\n|    1|   756|5151070044381239|2D010061220230612...|           68.0|\n|    2|   756|5151070044381239|2D010061220230612...|        3552.13|\n|    3|   756|5151070044381239|2D010061220230612...|            8.9|\n|    4|   756|5151070432093151|2D010061220230612...|           12.0|\n|    5|   756|5151070432093151|2D010061220230612...|           27.5|\n|    6|   756|5151070432093151|2D010061220230612...|           12.0|\n|    7|   756|5151070432093151|2D010061220230612...|           12.0|\n|    8|   756|5151940230409696|2D010061220230612...|         211.98|\n|    9|   756|5151940230409696|2D010061220230612...|          137.5|\n|   10|   756|5151070044387015|2D010061220230612...|          100.0|\n+-----+------+----------------+--------------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Criar uma SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Formatação de Arquivo CCB com Spark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Caminho do arquivo\n",
    "caminho_arquivo = \"/mnt/s3-ailos-teste/CEXT_7562011_20240125_0002504.CCB\"\n",
    "\n",
    "# Ler o arquivo como DataFrame\n",
    "df = spark.read.text(caminho_arquivo)\n",
    "\n",
    "# Adicionar uma coluna de linha numerada\n",
    "df = df.withColumn(\"linha\", col(\"value\"))\n",
    "\n",
    "# Extrair o conteúdo como uma lista de strings\n",
    "linhas = df.select(col(\"value\")).rdd.map(lambda row: row.value).collect()\n",
    "\n",
    "# Lista para armazenar os dados formatados\n",
    "dados_formatados = []\n",
    "\n",
    "# Validar a estrutura do arquivo\n",
    "if linhas[0].startswith(\"CEXT0\") and linhas[-1].startswith(\"CEXT9\"):\n",
    "    print(\"A estrutura do arquivo é válida.\")\n",
    "\n",
    "    # Processar as linhas do arquivo\n",
    "    for i, linha in enumerate(linhas[1:-1], start=1):  # Ignorar a primeira linha (cabeçalho) e a última linha (trailer)\n",
    "        # Separar os campos da linha\n",
    "        campos = linha.strip().split()\n",
    "\n",
    "        # Código\n",
    "        codigo = campos[0][:3]\n",
    "\n",
    "        # Número do cartão\n",
    "        numero_cartao = campos[0][3:].lstrip('0')\n",
    "\n",
    "        # Informações do arquivo\n",
    "        inf_arquivo = campos[1]\n",
    "\n",
    "        # Valor da transação\n",
    "        valor_transacao = float(campos[2][:11]) * 10 ** -2\n",
    "\n",
    "        # Adicionar os dados formatados à lista\n",
    "        dados_formatados.append((i, codigo, numero_cartao, inf_arquivo, round(valor_transacao, 2)))\n",
    "\n",
    "    # Criar DataFrame a partir dos dados formatados\n",
    "    df_formatado = spark.createDataFrame(dados_formatados, schema=[\"linha\", \"codigo\", \"numero_cartao\", \"inf_arquivo\", \"Valor_transacao\"])\n",
    "\n",
    "    # Mostrar os dados formatados\n",
    "    df_formatado.show()\n",
    "\n",
    "else:\n",
    "    print(\"A estrutura do arquivo não é válida.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de635a04-20d8-418b-9a44-420e72ad536d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A estrutura do arquivo é válida.\n+-----+------+----------------+--------------------+---------------+-----+\n|linha|codigo|   numero_cartao|         inf_arquivo|Valor_transacao|Ailos|\n+-----+------+----------------+--------------------+---------------+-----+\n|    1|   756|5151070044381239|2D010061220230612...|           68.0| Ailo|\n|    2|   756|5151070044381239|2D010061220230612...|        3552.13| Ailo|\n|    3|   756|5151070044381239|2D010061220230612...|            8.9| Ailo|\n|   10|   756|5151070044387015|2D010061220230612...|          100.0| Ailo|\n+-----+------+----------------+--------------------+---------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Criar uma SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Formatação de Arquivo CCB com Spark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Caminho do arquivo\n",
    "caminho_arquivo = \"/mnt/s3-ailos-teste/CEXT_7562011_20240125_0002504.CCB\"\n",
    "\n",
    "# Ler o arquivo como DataFrame\n",
    "df = spark.read.text(caminho_arquivo)\n",
    "\n",
    "# Adicionar uma coluna de linha numerada\n",
    "df = df.withColumn(\"linha\", col(\"value\"))\n",
    "\n",
    "# Extrair o conteúdo como uma lista de strings\n",
    "linhas = df.select(col(\"value\")).rdd.map(lambda row: row.value).collect()\n",
    "\n",
    "# Lista para armazenar os dados formatados\n",
    "dados_formatados = []\n",
    "\n",
    "# Lista de números de cartão da Ailos\n",
    "numeros_ailos = ['5151070044387015', '5151070044381239']  # Adicione aqui todos os números de cartão da Ailos\n",
    "\n",
    "# Função para verificar se o número do cartão é da Ailos\n",
    "def verificar_ailos(numero_cartao):\n",
    "    return \"Ailo\" if numero_cartao in numeros_ailos else \"Outro\"\n",
    "\n",
    "# Registrar a função como uma UDF (User Defined Function)\n",
    "verificar_ailos_udf = udf(verificar_ailos, StringType())\n",
    "\n",
    "# Validar a estrutura do arquivo\n",
    "if linhas[0].startswith(\"CEXT0\") and linhas[-1].startswith(\"CEXT9\"):\n",
    "    print(\"A estrutura do arquivo é válida.\")\n",
    "\n",
    "    # Processar as linhas do arquivo\n",
    "    for i, linha in enumerate(linhas[1:-1], start=1):  # Ignorar a primeira linha (cabeçalho) e a última linha (trailer)\n",
    "        # Separar os campos da linha\n",
    "        campos = linha.strip().split()\n",
    "\n",
    "        # Código\n",
    "        codigo = campos[0][:3]\n",
    "\n",
    "        # Número do cartão\n",
    "        numero_cartao = campos[0][3:].lstrip('0')\n",
    "\n",
    "        # Informações do arquivo\n",
    "        inf_arquivo = campos[1]\n",
    "\n",
    "        # Valor da transação\n",
    "        valor_transacao = float(campos[2][:11]) * 10 ** -2\n",
    "\n",
    "        # Adicionar os dados formatados à lista\n",
    "        dados_formatados.append((i, codigo, numero_cartao, inf_arquivo, round(valor_transacao, 2)))\n",
    "\n",
    "    # Criar DataFrame a partir dos dados formatados\n",
    "    df_formatado = spark.createDataFrame(dados_formatados, schema=[\"linha\", \"codigo\", \"numero_cartao\", \"inf_arquivo\", \"Valor_transacao\"])\n",
    "\n",
    "    # Adicionar coluna indicando se o número do cartão é da Ailos ou não\n",
    "    df_formatado = df_formatado.withColumn(\"Ailos\", verificar_ailos_udf(col(\"numero_cartao\")))\n",
    "\n",
    "    # Filtrar apenas os cartões da Ailos\n",
    "    df_formatado_ailos = df_formatado.filter(col(\"Ailos\") == \"Ailo\")\n",
    "\n",
    "    # Mostrar os dados formatados\n",
    "    df_formatado_ailos.show()\n",
    "\n",
    "else:\n",
    "    print(\"A estrutura do arquivo não é válida.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "291f34db-0ba6-4e9a-ac3f-d4ce63cd4d3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------------+--------------------+---------------+-----+--------------------+\n|linha|codigo|   numero_cartao|         inf_arquivo|Valor_transacao|Ailos|            IDCARTAO|\n+-----+------+----------------+--------------------+---------------+-----+--------------------+\n|    1|   756|5151070044381239|2D010061220230612...|           68.0| Ailo|FF95CEC4CBFC2F48E...|\n|    2|   756|5151070044381239|2D010061220230612...|        3552.13| Ailo|FF95CEC4CBFC2F48E...|\n|    3|   756|5151070044381239|2D010061220230612...|            8.9| Ailo|FF95CEC4CBFC2F48E...|\n|   10|   756|5151070044387015|2D010061220230612...|          100.0| Ailo|FF95CEC4CBF82F48E...|\n+-----+------+----------------+--------------------+---------------+-----+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Carregar os dados da tabela TB_CARTAO como um DataFrame\n",
    "df_cartao = spark.read.format(\"delta\").load(\"/mnt/s3-ailos-teste/TB_CARTAO\")\n",
    "\n",
    "# Selecionar as colunas NRCARTAO e IDCARTAO e renomear a coluna IDCARTAO\n",
    "df_cartao_id = df_cartao.select(col(\"NRCARTAO\").alias(\"NRCARTAO_TB\"), col(\"IDCARTAO\").alias(\"IDCARTAO_TB_CARTAO\"))\n",
    "\n",
    "# Realizar uma junção entre o DataFrame formatado e os dados da tabela TB_CARTAO\n",
    "df_formatado_com_idcartao = df_formatado.join(df_cartao_id, df_formatado.numero_cartao == df_cartao_id.NRCARTAO_TB, \"left\")\n",
    "\n",
    "# Adicionar a coluna IDCARTAO correspondente ao DataFrame formatado\n",
    "df_formatado_com_idcartao = df_formatado_com_idcartao.withColumn(\"IDCARTAO\", when(col(\"numero_cartao\") == col(\"NRCARTAO_TB\"), col(\"IDCARTAO_TB_CARTAO\")).otherwise(None))\n",
    "\n",
    "# Selecionar todas as colunas, exceto NRCARTAO_TB e IDCARTAO_TB_CARTAO\n",
    "df_final = df_formatado_com_idcartao.drop(\"NRCARTAO_TB\", \"IDCARTAO_TB_CARTAO\")\n",
    "\n",
    "# Filtrar apenas os registros que correspondem ao Ailos\n",
    "df_final_ailos = df_final.filter(col(\"Ailos\") == \"Ailo\")\n",
    "\n",
    "# Mostrar os dados formatados com a coluna IDCARTAO para os registros do Ailos\n",
    "df_final_ailos.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1240ef8-22dd-452a-913f-11377dbb76a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------------+--------------------+---------------+-----+--------------------+--------+\n|linha|codigo|   numero_cartao|         inf_arquivo|Valor_transacao|Ailos|            IDCARTAO|ID_TODOS|\n+-----+------+----------------+--------------------+---------------+-----+--------------------+--------+\n|    1|   756|5151070044381239|2D010061220230612...|           68.0| Ailo|FF95CEC4CBFC2F48E...|      A2|\n|    2|   756|5151070044381239|2D010061220230612...|        3552.13| Ailo|FF95CEC4CBFC2F48E...|      A2|\n|    3|   756|5151070044381239|2D010061220230612...|            8.9| Ailo|FF95CEC4CBFC2F48E...|      A2|\n|   10|   756|5151070044387015|2D010061220230612...|          100.0| Ailo|FF95CEC4CBF82F48E...|      A1|\n+-----+------+----------------+--------------------+---------------+-----+--------------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Supondo que você já tenha carregado os dados da tabela TB_CARTAO como um DataFrame chamado df_cartao\n",
    "\n",
    "# Carregar os dados da tabela TB_CARTAO como um DataFrame\n",
    "df_cartao = spark.read.format(\"delta\").load(\"/mnt/s3-ailos-teste/TB_CARTAO\")\n",
    "\n",
    "# Selecionar as colunas NRCARTAO e IDCARTAO e renomear a coluna IDCARTAO\n",
    "df_cartao_id = df_cartao.select(col(\"NRCARTAO\").alias(\"NRCARTAO_TB\"), col(\"IDCARTAO\").alias(\"IDCARTAO_TB_CARTAO\"), col(\"ID_TODOS\").alias(\"ID_TODOS_TB\"))\n",
    "\n",
    "# Realizar uma junção entre o DataFrame formatado e os dados da tabela TB_CARTAO\n",
    "df_formatado_com_idcartao = df_formatado.join(df_cartao_id, df_formatado.numero_cartao == df_cartao_id.NRCARTAO_TB, \"left\")\n",
    "\n",
    "# Adicionar a coluna IDCARTAO correspondente ao DataFrame formatado\n",
    "df_formatado_com_idcartao = df_formatado_com_idcartao.withColumn(\"IDCARTAO\", when(col(\"numero_cartao\") == col(\"NRCARTAO_TB\"), col(\"IDCARTAO_TB_CARTAO\")).otherwise(None))\n",
    "\n",
    "# Adicionar a coluna ID_TODOS correspondente ao DataFrame formatado\n",
    "df_formatado_com_idcartao = df_formatado_com_idcartao.withColumn(\"ID_TODOS\", when(col(\"numero_cartao\") == col(\"NRCARTAO_TB\"), col(\"ID_TODOS_TB\")).otherwise(None))\n",
    "\n",
    "# Selecionar todas as colunas, exceto NRCARTAO_TB, IDCARTAO_TB_CARTAO e ID_TODOS_TB\n",
    "df_final = df_formatado_com_idcartao.drop(\"NRCARTAO_TB\", \"IDCARTAO_TB_CARTAO\", \"ID_TODOS_TB\")\n",
    "\n",
    "# Filtrar apenas os registros que correspondem ao Ailos\n",
    "df_final_ailos = df_final.filter(col(\"Ailos\") == \"Ailo\")\n",
    "\n",
    "# Mostrar os dados formatados com as colunas IDCARTAO e ID_TODOS para os registros do Ailos\n",
    "df_final_ailos.show()\n",
    "\n",
    "# Escrever o DataFrame sem duplicatas para o Delta Lake\n",
    "caminho_dados_df_final_ailos = \"/mnt/s3-ailos-teste/silver/_delta_log/TB_CARTAO\"\n",
    "df_final_ailos.write.format(\"delta\").mode(\"overwrite\").save(caminho_dados_df_final_ailos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "931ffe5a-f7ad-4543-8f9c-98d93fdd5208",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------------+--------------------+---------------+-----+--------------------+--------+------------------+--------------+\n|linha|codigo|   numero_cartao|         inf_arquivo|Valor_transacao|Ailos|            IDCARTAO|ID_TODOS|IDARQUIVO_CONTROLE|     IDARQUIVO|\n+-----+------+----------------+--------------------+---------------+-----+--------------------+--------+------------------+--------------+\n|    1|   756|5151070044381239|2D010061220230612...|           68.0| Ailo|FF95CEC4CBFC2F48E...|      A2|                 3| 9999999888888|\n|    2|   756|5151070044381239|2D010061220230612...|        3552.13| Ailo|FF95CEC4CBFC2F48E...|      A2|                 3| 9999999888887|\n|    3|   756|5151070044381239|2D010061220230612...|            8.9| Ailo|FF95CEC4CBFC2F48E...|      A2|                 3| 9999999888886|\n|   10|   756|5151070044387015|2D010061220230612...|          100.0| Ailo|FF95CEC4CBF82F48E...|      A1|                 2|99999998888889|\n+-----+------+----------------+--------------------+---------------+-----+--------------------+--------+------------------+--------------+\n\n+-----+------+----------------+--------------------+---------------+-----+--------------------+--------+------------------+--------------+\n|linha|codigo|   numero_cartao|         inf_arquivo|Valor_transacao|Ailos|            IDCARTAO|ID_TODOS|IDARQUIVO_CONTROLE|     IDARQUIVO|\n+-----+------+----------------+--------------------+---------------+-----+--------------------+--------+------------------+--------------+\n|    1|   756|5151070044381239|2D010061220230612...|           68.0| Ailo|FF95CEC4CBFC2F48E...|      A2|                 3| 9999999888888|\n|    2|   756|5151070044381239|2D010061220230612...|        3552.13| Ailo|FF95CEC4CBFC2F48E...|      A2|                 3| 9999999888887|\n|    3|   756|5151070044381239|2D010061220230612...|            8.9| Ailo|FF95CEC4CBFC2F48E...|      A2|                 3| 9999999888886|\n|   10|   756|5151070044387015|2D010061220230612...|          100.0| Ailo|FF95CEC4CBF82F48E...|      A1|                 2|99999998888889|\n+-----+------+----------------+--------------------+---------------+-----+--------------------+--------+------------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Inicializar a sessão do Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Adicionar IDARQUIVO_CONTROLE\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Carregar os dados da tabela TB_ARQUIVO_CONTROLE como um DataFrame\n",
    "df_arquivo_controle = spark.read.format(\"delta\").load(\"/mnt/s3-ailos-teste/TB_ARQUIVO_CONTROLE\")\n",
    "\n",
    "# Selecionar as colunas relevantes e renomeá-las se necessário\n",
    "df_arquivo_controle_id = df_arquivo_controle.select(col(\"NRARQUIVO\").alias(\"NRARQUIVO_TB\"), col(\"ID_TODOS\").alias(\"ID_TODOS_TB_ARQUIVO\"))\n",
    "\n",
    "# Realizar uma junção entre o DataFrame existente e os dados da tabela TB_ARQUIVO_CONTROLE\n",
    "df_final_com_nrarquivo = df_final_ailos.join(df_arquivo_controle_id, df_final_ailos.ID_TODOS == df_arquivo_controle_id.ID_TODOS_TB_ARQUIVO, \"left\")\n",
    "\n",
    "# Adicionar a coluna NRARQUIVO correspondente ao DataFrame final\n",
    "df_final_com_nrarquivo = df_final_com_nrarquivo.withColumn(\"IDARQUIVO_CONTROLE\", when(col(\"ID_TODOS\") == col(\"ID_TODOS_TB_ARQUIVO\"), col(\"NRARQUIVO_TB\")).otherwise(None))\n",
    "\n",
    "# Selecionar todas as colunas relevantes\n",
    "df_final_completo = df_final_com_nrarquivo.drop(\"NRARQUIVO_TB\", \"ID_TODOS_TB_ARQUIVO\")\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Inicializar a sessão do Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Adicionar Coluna\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define a função para gerar o IDARQUIVO com base no padrão especificado\n",
    "def generate_idarquivo(id_todos):\n",
    "    prefixo = \"999999988888\"\n",
    "    \n",
    "    # Garante que id_todos seja uma string\n",
    "    id_todos_str = str(id_todos)\n",
    "    \n",
    "    # Remove caracteres não numéricos e pega os últimos 6 caracteres\n",
    "    id_todos_numerico = ''.join(filter(str.isdigit, id_todos_str))[-6:]\n",
    "    \n",
    "    # Gera o sufixo usando os caracteres numéricos restantes\n",
    "    sufixo = \"\".join(str(9 - int(i)) for i in id_todos_numerico)\n",
    "    \n",
    "    return prefixo + sufixo\n",
    "\n",
    "# Registra a função como um UDF\n",
    "generate_idarquivo_udf = udf(generate_idarquivo, StringType())\n",
    "\n",
    "# Adiciona a coluna IDARQUIVO ao DataFrame usando a função e a coluna de informações únicas de cada linha\n",
    "df_final_completo = df_final_completo.withColumn(\"IDARQUIVO\", generate_idarquivo_udf(col(\"linha\")))\n",
    "\n",
    "# Mostra os dados finais\n",
    "df_final_completo.show()\n",
    "\n",
    "# Caminho para salvar o DataFrame em formato Parquet\n",
    "caminho_salvar_parquet = \"/mnt/s3-ailos-teste/df_final_completo_parquet\"\n",
    "\n",
    "# Salvar o DataFrame em formato Parquet\n",
    "df_final_completo.write.mode(\"overwrite\").parquet(caminho_salvar_parquet)\n",
    "\n",
    "# Mostrar os dados finais\n",
    "df_final_completo.show()\n",
    "\n",
    "caminho_dados_df_final_completo = \"/mnt/s3-ailos-teste/silver/_delta_log/TB_ARQUIVO_CONTROLE\"\n",
    "df_final_completo.write.format(\"delta\").mode(\"overwrite\").save(caminho_dados_df_final_completo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "892e4f2a-573c-4f50-b06e-3ae774b4291f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data formatada: 25/01/2024\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Criar uma sessão Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Arquivo CCB\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Caminho para o arquivo de exemplo\n",
    "caminho_arquivo_1 = \"/mnt/s3-ailos-teste/CEXT_7562011_20240125_0002504.CCB\"\n",
    "\n",
    "# Ler a primeira linha e primeira coluna do arquivo\n",
    "primeiro_valor = spark.read.text(caminho_arquivo_1).selectExpr(\"split(value, '\\t')[0] as primeira_coluna\").first().primeira_coluna\n",
    "\n",
    "# Tratar o valor\n",
    "valor_tratado = primeiro_valor[8:-7]\n",
    "\n",
    "# Converter para o formato DD/MM/AAAA\n",
    "data_formatada = valor_tratado[6:] + \"/\" + valor_tratado[4:6] + \"/\" + valor_tratado[:4]\n",
    "\n",
    "# Mostrar a data formatada\n",
    "print(\"Data formatada:\", data_formatada)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4ab15b0-07cc-40a6-bcf5-4983f0c7bd64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+----------------+----------+----------+\n|     IDARQUIVO|IDARQUIVO_CONTROLE|       NMARQUIVO| DTARQUIVO|DHREGISTRO|\n+--------------+------------------+----------------+----------+----------+\n| 9999999888886|                 3|5151070044381239|2024-05-08|2024-05-07|\n| 9999999888887|                 3|5151070044381239|2024-05-08|2024-05-07|\n| 9999999888888|                 3|5151070044381239|2024-05-08|2024-05-07|\n|99999998888889|                 2|5151070044387015|2024-05-08|2024-05-07|\n+--------------+------------------+----------------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, to_date\n",
    "\n",
    "# Inicializar a sessão do Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CARTOES.TB_ARQUIVO\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Selecionar as colunas desejadas e criar um novo DataFrame\n",
    "novo_df = df_final_completo.select(\"IDARQUIVO\", \"IDARQUIVO_CONTROLE\", \"numero_cartao\")\n",
    "\n",
    "# Renomear a coluna \"numero_cartao\" para \"NMARQUIVO\"\n",
    "novo_df = novo_df.withColumnRenamed(\"numero_cartao\", \"NMARQUIVO\")\n",
    "\n",
    "# Adicionar a data \"data_formatada\" em todas as linhas\n",
    "data_formatada = \"2024-05-08\"\n",
    "novo_df = novo_df.withColumn(\"DTARQUIVO\", to_date(lit(data_formatada)))  # Criar a coluna DTARQUIVO com base na data_formatada\n",
    "# Especificar o caminho onde os dados serão armazenados\n",
    "caminho_dados_arquivo_controle = \"/mnt/s3-ailos-teste/TB_ARQUIVO_CONTROLE\"\n",
    "# Ler a coluna DHREGISTRO do DataFrame df_arquivo_controle\n",
    "dh_registro = spark.read.format(\"delta\").load(caminho_dados_arquivo_controle).select(\"DHREGISTRO\")\n",
    "\n",
    "# Adicionar a coluna DHREGISTRO ao DataFrame novo_df\n",
    "novo_df = novo_df.join(dh_registro)\n",
    "\n",
    "# Remover duplicatas com base na coluna \"IDARQUIVO\"\n",
    "novo_df_sem_duplicatas = novo_df.dropDuplicates([\"IDARQUIVO\"])\n",
    "\n",
    "# Mostrar os dados do DataFrame sem duplicatas\n",
    "novo_df_sem_duplicatas.show()\n",
    "\n",
    "# Escrever o DataFrame sem duplicatas para o Delta Lake\n",
    "caminho_dados_silver_sem_duplicatas = \"/mnt/s3-ailos-teste/silver/_delta_log/TB_ARQUIVO\"\n",
    "novo_df_sem_duplicatas.write.format(\"delta\").mode(\"overwrite\").save(caminho_dados_silver_sem_duplicatas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e78a955-af6d-460f-9838-dc5ddbbc0f23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+-------+----------+----------+----------+\n|     IDARQUIVO|  IDARQUIVO_LINHA|NRLINHA|DSCONTEUDO|DTPROCESSO|CDSITUACAO|\n+--------------+-----------------+-------+----------+----------+----------+\n| 9999999888886|11111111111111111|      2| conteudo2|2024-05-07|         1|\n| 9999999888887|11111111111111112|      2| conteudo2|2024-05-07|         1|\n| 9999999888888|11111111111111113|      2| conteudo2|2024-05-07|         1|\n|99999998888889|11111111111111114|      1| conteudo1|2024-05-07|         1|\n+--------------+-----------------+-------+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, lit\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Definir uma janela de partição para a função row_number\n",
    "window = Window.orderBy(\"IDARQUIVO\")\n",
    "\n",
    "# Adicionar uma coluna com um número de linha sequencial\n",
    "novo_df_sem_duplicatas = novo_df_sem_duplicatas.withColumn(\"row_id\", row_number().over(window))\n",
    "\n",
    "# Converter o número de linha em uma string de 16 caracteres repetidos\n",
    "novo_df_sem_duplicatas = novo_df_sem_duplicatas.withColumn(\"IDARQUIVO_LINHA\", concat(lit(\"1\" * 16), novo_df_sem_duplicatas[\"row_id\"].cast(\"string\")))\n",
    "\n",
    "# Selecionar apenas as colunas desejadas\n",
    "linha_df = novo_df_sem_duplicatas.select(\"IDARQUIVO_LINHA\", \"IDARQUIVO\")\n",
    "\n",
    "# Importar df_final_completo\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Adicionando a coluna ID_TODOS de df_final_completo a linha_df\n",
    "linha_df_com_id_todos = linha_df.join(df_final_completo.select(\"IDARQUIVO\", \"ID_TODOS\"), \"IDARQUIVO\", \"left\")\n",
    "\n",
    "# Lendo a tabela caminho_dados_arquivo_linha usando o formato Delta\n",
    "caminho_dados_arquivo_linha = spark.read.format(\"delta\").load(\"/mnt/s3-ailos-teste/TB_ARQUIVO_LINHA\")\n",
    "\n",
    "# Adicionando as colunas DSCONTEUDO, DTPROCESSO e CDSITUACAO a linha_df_com_id_todos\n",
    "linha_df_com_nrlinha_e_colunas = (\n",
    "    linha_df_com_id_todos\n",
    "    .join(caminho_dados_arquivo_linha.select(\"ID_TODOS\", \"NRLINHA\", \"DSCONTEUDO\", \"DTPROCESSO\", \"CDSITUACAO\"), \"ID_TODOS\", \"left\")\n",
    "    .drop(\"ID_TODOS\")  # Removendo a coluna ID_TODOS se não for necessária\n",
    ")\n",
    "\n",
    "# Mostrando os dados do DataFrame linha_df_com_nrlinha_e_colunas\n",
    "linha_df_com_nrlinha_e_colunas.show()\n",
    "\n",
    "# Escrever o DataFrame sem duplicatas para o Delta Lake\n",
    "caminho_dados_linha_df_com_nrlinha_e_colunas = \"/mnt/s3-ailos-teste/silver/_delta_log/TB_ARQUIVO_LINHA\"\n",
    "linha_df_com_nrlinha_e_colunas.write.format(\"delta\").mode(\"overwrite\").save(caminho_dados_linha_df_com_nrlinha_e_colunas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08c770b1-0bd9-429b-9bc1-afd089a00fb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definir o caminho do arquivo\n",
    "file_path = \"/dbfs/mnt/s3-ailos-teste/gold/arquivo.sql\"\n",
    "\n",
    "# Código SQL a ser escrito no arquivo\n",
    "sql_code = \"\"\"\n",
    "-- Criar a tabela TB_ARQUIVO\n",
    "CREATE TABLE CARTOES.TB_ARQUIVO (\n",
    "  IDARQUIVO RAW(16) NOT NULL,\n",
    "  IDARQUIVO_CONTROLE RAW(16) NOT NULL,\n",
    "  NMARQUIVO VARCHAR(100) NOT NULL,\n",
    "  DTARQUIVO DATE,\n",
    "  DHREGISTRO DATE NOT NULL,\n",
    "  CONSTRAINT TB_ARQUIVO_PK PRIMARY KEY (IDARQUIVO)\n",
    ")\n",
    "TABLESPACE TBS_CARTOES_D\n",
    "PCTFREE 10\n",
    "INITRANS 1\n",
    "MAXTRANS 255\n",
    "STORAGE (\n",
    "  INITIAL 64K\n",
    "  NEXT 1M\n",
    "  MINEXTENTS 1\n",
    "  MAXEXTENTS UNLIMITED\n",
    ")\n",
    "COMPRESS FOR ALL OPERATIONS;\n",
    "\n",
    "\n",
    "COMMENT ON TABLE CARTOES.TB_ARQUIVO IS 'Propósito: Tabela de Arquivos do produto CARTAO. Finalidade: Tabela para armazenar informações dos arquivos gerados ou recebidos. Volumetria: Inicial de X registros, com uma taxa de crescimento de Y% ao mês. Política de Expurgo: Z meses de retenção.';\n",
    "COMMENT ON COLUMN CARTOES.TB_ARQUIVO.IDARQUIVO IS 'Código único do arquivo. #CLASSIFICAÇÃO_DADO: I';\n",
    "COMMENT ON COLUMN CARTOES.TB_ARQUIVO.IDARQUIVO_CONTROLE IS 'Código único de controle do arquivo. #CLASSIFICAÇÃO_DADO: I';\n",
    "COMMENT ON COLUMN CARTOES.TB_ARQUIVO.NMARQUIVO IS 'Nome do arquivo. #CLASSIFICAÇÃO_DADO: I';\n",
    "COMMENT ON COLUMN CARTOES.TB_ARQUIVO.DTARQUIVO IS 'Data do arquivo. #CLASSIFICAÇÃO_DADO: I';\n",
    "COMMENT ON COLUMN CARTOES.TB_ARQUIVO.DHREGISTRO IS 'Data/Hora de registro na base de dados. #CLASSIFICAÇÃO_DADO: I';\n",
    "\n",
    "-- Inserir os dados do DataFrame no banco de dados\n",
    "INSERT INTO CARTOES.TB_ARQUIVO (IDARQUIVO, IDARQUIVO_CONTROLE, NMARQUIVO, DTARQUIVO, DHREGISTRO)\n",
    "VALUES \n",
    "('9999999888886', '3', '5151070044381239', TO_DATE('2024-05-08', 'YYYY-MM-DD'), TO_DATE('2024-05-07', 'YYYY-MM-DD')),\n",
    "('9999999888887', '3', '5151070044381239', TO_DATE('2024-05-08', 'YYYY-MM-DD'), TO_DATE('2024-05-07', 'YYYY-MM-DD')),\n",
    "('9999999888888', '3', '5151070044381239', TO_DATE('2024-05-08', 'YYYY-MM-DD'), TO_DATE('2024-05-07', 'YYYY-MM-DD')),\n",
    "('9999999888889', '2', '5151070044387015', TO_DATE('2024-05-08', 'YYYY-MM-DD'), TO_DATE('2024-05-07', 'YYYY-MM-DD'));\n",
    "\n",
    "-- Criar a tabela TB_ARQUIVO_LINHA\n",
    "CREATE TABLE CARTOES.TB_ARQUIVO_LINHA (\n",
    "  IDARQUIVO RAW(16) NOT NULL,\n",
    "  IDARQUIVO_LINHA VARCHAR(16) NOT NULL,\n",
    "  NRLINHA INT,\n",
    "  DSCONTEUDO VARCHAR(100),\n",
    "  DTPROCESSO DATE,\n",
    "  CDSITUACAO INT,\n",
    "  CONSTRAINT TB_ARQUIVO_LINHA_PK PRIMARY KEY (IDARQUIVO, IDARQUIVO_LINHA),\n",
    "  CONSTRAINT FK_TB_ARQUIVO FOREIGN KEY (IDARQUIVO) REFERENCES CARTOES.TB_ARQUIVO(IDARQUIVO)\n",
    ")\n",
    "TABLESPACE TBS_CARTOES_D\n",
    "PCTFREE 10\n",
    "INITRANS 1\n",
    "MAXTRANS 255\n",
    "STORAGE (\n",
    "  INITIAL 64K\n",
    "  NEXT 1M\n",
    "  MINEXTENTS 1\n",
    "  MAXEXTENTS UNLIMITED\n",
    ")\n",
    "COMPRESS FOR ALL OPERATIONS;\n",
    "\n",
    "COMMENT ON TABLE CARTOES.TB_ARQUIVO_LINHA IS 'Propósito: Tabela de Linhas do Arquivo do produto CARTAO. Finalidade: Tabela para armazenar informações detalhadas das linhas dos arquivos gerados ou recebidos. Volumetria: Inicial de X registros, com uma taxa de crescimento de Y% ao mês. Política de Expurgo: Z meses de retenção.';\n",
    "COMMENT ON COLUMN CARTOES.TB_ARQUIVO_LINHA.IDARQUIVO IS 'Chave estrangeira para a tabela TB_ARQUIVO. #CLASSIFICAÇÃO_DADO: I';\n",
    "COMMENT ON COLUMN CARTOES.TB_ARQUIVO_LINHA.IDARQUIVO_LINHA IS 'Identificador único da linha do arquivo. #CLASSIFICAÇÃO_DADO: I';\n",
    "COMMENT ON COLUMN CARTOES.TB_ARQUIVO_LINHA.NRLINHA IS 'Número da linha do arquivo. #CLASSIFICAÇÃO_DADO: I';\n",
    "COMMENT ON COLUMN CARTOES.TB_ARQUIVO_LINHA.DSCONTEUDO IS 'Descrição do conteúdo da linha. #CLASSIFICAÇÃO_DADO: I';\n",
    "COMMENT ON COLUMN CARTOES.TB_ARQUIVO_LINHA.DTPROCESSO IS 'Data de processamento da linha. #CLASSIFICAÇÃO_DADO: I';\n",
    "COMMENT ON COLUMN CARTOES.TB_ARQUIVO_LINHA.CDSITUACAO IS 'Código da situação da linha. #CLASSIFICAÇÃO_DADO: I';\n",
    "\n",
    "INSERT INTO CARTOES.TB_ARQUIVO_LINHA (IDARQUIVO, IDARQUIVO_LINHA, NRLINHA, DSCONTEUDO, DTPROCESSO, CDSITUACAO)\n",
    "VALUES \n",
    "('9999999888886', '11111111111111111', 2, 'conteudo2', TO_DATE('2024-05-07', 'YYYY-MM-DD'), 1),\n",
    "('9999999888887', '11111111111111112', 2, 'conteudo2', TO_DATE('2024-05-07', 'YYYY-MM-DD'), 1),\n",
    "('9999999888888', '11111111111111113', 2, 'conteudo2', TO_DATE('2024-05-07', 'YYYY-MM-DD'), 1),\n",
    "('9999999888889', '11111111111111114', 1, 'conteudo1', TO_DATE('2024-05-07', 'YYYY-MM-DD'), 1);\n",
    "\n",
    "-- Create table TB_ARQUIVO_CONTROLE\n",
    "CREATE TABLE CARTOES.TB_ARQUIVO_CONTROLE (\n",
    "  idarquivo_controle   RAW(16) DEFAULT SYS_GUID() NOT NULL,\n",
    "  nrarquivo            NUMBER(2) NOT NULL,\n",
    "  dtalteracao          DATE,\n",
    "  nrsequencial_arquivo NUMBER(10),\n",
    "  dsdiretorio_arquivo  VARCHAR2(50),\n",
    "  dhregistro           DATE DEFAULT SYSDATE NOT NULL\n",
    ");\n",
    "\n",
    "\n",
    "COMMENT ON TABLE CARTOES.TB_ARQUIVO_CONTROLE IS 'Proposito: Tabela de Controle para Arquivos de Integração do produto CARTAO. Finalidade: Tabela para armazenar parametros utilizados na integracao de arquivos com o parceiro, como diretorios utilizados e sequencial do arquivo. Volumetria: Inicial de 4 registros, com 0% ao mes. Politica de Expurgo: Nao se aplica.';\n",
    "COMMENT ON COLUMN CARTOES.TB_ARQUIVO_CONTROLE.idarquivo_controle IS 'Código único do Controle do Arquivo utilizado como Chave Primaria. #CLASSIFICACAO_DADO: I';\n",
    "COMMENT ON COLUMN CARTOES.TB_ARQUIVO_CONTROLE.nrarquivo IS 'Tipo do arquivo (Tabela de dominio = TB_DOMINIO_CAMPO). #CLASSIFICACAO_DADO: I';\n",
    "COMMENT ON COLUMN CARTOES.TB_ARQUIVO_CONTROLE.dtalteracao IS 'Data da Última alteração. #CLASSIFICACAO_DADO: I';\n",
    "COMMENT ON COLUMN CARTOES.TB_ARQUIVO_CONTROLE.nrsequencial_arquivo IS 'Número do último sequencial do arquivo. #CLASSIFICACAO_DADO: I';\n",
    "COMMENT ON COLUMN CARTOES.TB_ARQUIVO_CONTROLE.dsdiretorio_arquivo IS 'Caminho do diretório do arquivo. #CLASSIFICACAO_DADO: I';\n",
    "COMMENT ON COLUMN CARTOES.TB_ARQUIVO_CONTROLE.dhregistro IS 'Data/Hora de inclusao do registro. #CLASSIFICACAO_DADO: I';\n",
    "\n",
    "CREATE UNIQUE INDEX CARTOES.TB_ARQUIVO_CONTROLE_NEW_PK ON CARTOES.TB_ARQUIVO_CONTROLE (IDARQUIVO_CONTROLE);\n",
    "CREATE UNIQUE INDEX CARTOES.TB_ARQUIVO_CONTROLE_NEW_UK1 ON CARTOES.TB_ARQUIVO_CONTROLE (NRARQUIVO);\n",
    "\n",
    "ALTER TABLE CARTOES.TB_ARQUIVO_CONTROLE ADD CONSTRAINT TB_ARQUIVO_CONTROLE_PK PRIMARY KEY (IDARQUIVO_CONTROLE);\n",
    "\n",
    "ALTER TABLE CARTOES.TB_ARQUIVO_CONTROLE ADD CONSTRAINT TB_ARQUIVO_CONTROLE_CK1 CHECK (NRARQUIVO BETWEEN 1 AND 99);\n",
    "\n",
    "INSERT INTO CARTOES.TB_ARQUIVO_CONTROLE (IDARQUIVO_CONTROLE, NRARQUIVO, DTALTERACAO, NRSEQUENCIAL_ARQUIVO, DSDIRETORIO_ARQUIVO, DHREGISTRO)\n",
    "VALUES ('FF95CEC4CE732F48E0530100007FBDE5', 2, SYSDATE, 3731, '/temp', SYSDATE);\n",
    "\n",
    "INSERT INTO CARTOES.TB_ARQUIVO_CONTROLE (IDARQUIVO_CONTROLE, NRARQUIVO, DTALTERACAO, NRSEQUENCIAL_ARQUIVO, DSDIRETORIO_ARQUIVO, DHREGISTRO)\n",
    "VALUES ('FF95CEC4D31A2F48E0530100007FBDE5', 3, SYSDATE, 3131, '/recebe', SYSDATE);\n",
    "\n",
    "COMMIT;\n",
    "\"\"\"\n",
    "\n",
    "# Escrever o código SQL no arquivo\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(sql_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d5f1928-44cc-4b96-8b21-7e44737bf930",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "curl -X POST -H \"Authorization: Bearer dapi32486fed3330feaef794007d0725b2ff\" -H \"Content-Type: application/json\" -d '{\n",
    "  \"job_id\": \"309361002103294\"\n",
    "}' \"https://dbc-990df228-8a13.cloud.databricks.com/api/2.0/jobs/run-now\"\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
